{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661d37da",
   "metadata": {},
   "source": [
    "## MeLi Data Challenge 2019\n",
    "\n",
    "This notebook is part of a curated version of my original solution for the MeLi Data Challenge hosted by [Mercado Libre](https://www.mercadolibre.com/) in 2019\n",
    "\n",
    "The goal of this first challenge was to create a model that would classify items into categories based solely on the item’s title. \n",
    "\n",
    "This title is a free text input from the seller that would become the header of the listings.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note</b> <p>Only 10% of the data is used in the notebooks to improve the experience.</p>\n",
    "    <p>Also, only spanish data is used in this notebooks for simplicity reasons only</p>\n",
    "    <p>In the scripted version, 100% of the data is used to improve results</p>\n",
    "</div>\n",
    "\n",
    "### 2 - PreProcess\n",
    "\n",
    "In this notebook I'm collecting all the pre-processing steps and alternatives applied to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0d1af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Basla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unidecode\n",
    "import re\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd6470",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "It can be downloaded from here:\n",
    "- [train](https://meli-data-challenge.s3.amazonaws.com/train.csv.gz)\n",
    "\n",
    "- [test](https://meli-data-challenge.s3.amazonaws.com/test.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e05e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv('data/train.csv.gz', compression='gzip')\n",
    "train_data = pd.read_csv('./../data/sample_train.csv')\n",
    "test_data = pd.read_csv('./../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c17590b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>label_quality</th>\n",
       "      <th>language</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Riñonera En Oferta Con Un Gran Espacio</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>FANNY_PACKS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Uxcell Detector De Interruptor De Sensor De Pr...</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>ALARMS_AND_SENSORS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Kit 8 Botadores Hidraulicos Ford Transit 2.4 Tdci</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>ENGINE_TAPPET_GUIDE_HOLDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bolso Bajo Asiento, Bicicleta, Ciclismo, Noaf.</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>BICYCLE_BAGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Funda Para Almohadon C Pelo Y Cierre</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>CUSHION_COVERS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0             Riñonera En Oferta Con Un Gran Espacio   \n",
       "1           1  Uxcell Detector De Interruptor De Sensor De Pr...   \n",
       "2           2  Kit 8 Botadores Hidraulicos Ford Transit 2.4 Tdci   \n",
       "3           3     Bolso Bajo Asiento, Bicicleta, Ciclismo, Noaf.   \n",
       "4           4               Funda Para Almohadon C Pelo Y Cierre   \n",
       "\n",
       "  label_quality language                   category  \n",
       "0    unreliable  spanish                FANNY_PACKS  \n",
       "1    unreliable  spanish         ALARMS_AND_SENSORS  \n",
       "2    unreliable  spanish  ENGINE_TAPPET_GUIDE_HOLDS  \n",
       "3    unreliable  spanish               BICYCLE_BAGS  \n",
       "4    unreliable  spanish             CUSHION_COVERS  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b2087",
   "metadata": {},
   "source": [
    "### Clean titles\n",
    "\n",
    "Many of the following steps should be run separately for each language.\n",
    "\n",
    "Here, we run all together, but in the final script data will be splitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf405a9a",
   "metadata": {},
   "source": [
    "**Step 1**: run each title through unidecode to remove special characters present in spanish and portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1644c3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original text: Riñonera En Oferta Con Un Gran Espacio\n",
      "Processed text: Rinonera En Oferta Con Un Gran Espacio\n",
      "\n",
      " Original text: Colchón Doble Inflable 191x137x22cm\n",
      "Processed text: Colchon Doble Inflable 191x137x22cm\n"
     ]
    }
   ],
   "source": [
    "for text in train_data.iloc[[0, 6]].title.values:\n",
    "    print(f\"\\n Original text: {text}\")\n",
    "    print(f\"Processed text: {unidecode.unidecode(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd593498",
   "metadata": {},
   "source": [
    "**Step 2**: Clean special cases using RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb48b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original text: Cuerina P/tapiceria -  Caramelo 29 - Mostaza\n",
      "Processed text: Cuerina P/tapiceria - Caramelo 29 - Mostaza\n",
      "\n",
      " Original text:  Bateria  Moura M22gd  Blindada Para Nissan Acenta 12v X 65\n",
      "Processed text:  Bateria Moura M22gd Blindada Para Nissan Acenta 12v X 65\n"
     ]
    }
   ],
   "source": [
    "# Remove extra spaces and characters repeated more than 3 times in a row\n",
    "for string in train_data.iloc[[40, 101]].title.values:\n",
    "    stringA = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    stringB = re.sub(r'(.)\\1{3,}', r'\\1\\1', stringA)\n",
    "    if string != stringB:\n",
    "        print(f\"\\n Original text: {string}\")\n",
    "        print(f\"Processed text: {stringB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b226155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original text: Colchón Doble Inflable 191x137x22cm\n",
      "Processed text: Colchón Doble Inflable 191x137x <smeasure> \n",
      "\n",
      " Original text: Rollo De Vinilo Blanco Brillo Base Gris Ritrama De 1.05x50 M\n",
      "Processed text: Rollo De Vinilo Blanco Brillo Base Gris Ritrama De 1.05x <smeasure> \n",
      "\n",
      " Original text: Extension Hexagonal 230 Mm C/mecha Guia Bremen®\n",
      "Processed text: Extension Hexagonal  <smeasure>  C/mecha Guia Bremen®\n",
      "\n",
      " Original text: Puntilla De Nylon Blanca De 1 Cm X 10 Mts\n",
      "Processed text: Puntilla De Nylon Blanca De  <smeasure>  X  <smeasure> \n"
     ]
    }
   ],
   "source": [
    "# Find spatial measures and replace them with a special tag\n",
    "for string in train_data.head(50).title.values:\n",
    "    stringB = re.sub(r\"((\\d)+(\\,|\\.){0,1}(\\d)*( ){0,1}((mts*)|(pulgadas*)|('')|(polegadas*)|(m)|(mms*)|(cms*)|(metros*)|(mtrs*)|(centimetros*))+)(?!(?!x)[A-Za-z])\", \" <smeasure> \", string, flags=re.I)\n",
    "    if string != stringB:\n",
    "        print(f\"\\n Original text: {string}\")\n",
    "        print(f\"Processed text: {stringB}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c685c",
   "metadata": {},
   "source": [
    "**Step 3**\n",
    "Putting it all together (and adding a \"few\" more expressions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d828fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for datasets.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    \n",
    "    string = unidecode.unidecode(string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r'(.)\\1{3,}', r'\\1\\1', string)\n",
    "    \n",
    "    string = re.sub(r'\\d{7,}', r' <longnumber> ', string)\n",
    "    string = re.sub(r'\\d{7,}', r' <longnumber> ', string)\n",
    "    string = re.sub(r'(?<=[A-Za-z])[\\d,]{10,}', r' <model> ', string)\n",
    "    string = re.sub(r'[\\d,]{10,}', r' <weird> ', string)\n",
    "    \n",
    "    # Spatial\n",
    "    string = re.sub(r\"((\\d)+(\\,|\\.){0,1}(\\d)*( ){0,1}((mts*)|(pulgadas*)|('')|\"\n",
    "                    \"(polegadas*)|(m)|(mms*)|(cms*)|(metros*)|(mtrs*)|(centimetros*))+)\"\n",
    "                    \"(?!(?!x)[A-Za-z])\", \" <smeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(mts)+( ){0,1}((\\d)+(\\,|\\.){0,1}(\\d)*)(?![A-Za-z])\", \" <smeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"<smeasure> +[\\/x] +<smeasure> +[\\/x] +<smeasure>\", \" <smeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(<smeasure>|(\\d)+(\\,|\\.)*(\\d*)) +[\\/x] \"\n",
    "                    \"+(<smeasure>|(\\d)+(\\,|\\.)*(\\d*)) +[\\/x] +<smeasure>\", \" <smeasure> \", string, \n",
    "                    flags=re.I)\n",
    "    string = re.sub(r\"<smeasure> +[\\/x] +<smeasure>\", \" <smeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])(<smeasure>|(\\d)+(\\,|\\.)*(\\d*)) *[\\/x-] *<smeasure>\", \n",
    "                    \" <smeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*))x((\\d)+(\\,|\\.)*(\\d*))x +<smeasure>\", \" <smeasure> \", \n",
    "                    string, flags=re.I)\n",
    "    \n",
    "    # Electrical\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *amperes\", \" <emeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<!(?<![\\dx])[\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *amps*\", \" <emeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *mah\", \" <emeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *vol.\", \" <emeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"\\b(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *kw\\b\", \" <emeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"\\b(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *v+\\b\", \" <emeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *volts*\", \" <emeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *w(ts)*(?![\\dA-Za-z])\", \" <emeasure> \", \n",
    "                    string, flags=re.I)\n",
    "    \n",
    "    # Pressure\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *psi\", \" <pmeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *bar\", \" <pmeasure> \", string, flags=re.I)\n",
    "    \n",
    "    # Weights\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *kgs*\", \" <wmeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *kilos*\", \" <wmeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"\\b(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *g\\b\", \" <wmeasure> \", string, flags=re.I)\n",
    "    \n",
    "    # IT\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *tb\", \" <itmeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *gb\", \" <itmeasure> \", string, flags=re.I)\n",
    "    \n",
    "    # Volume\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *cc(?![0-9])\", \" <vmeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *litros*\", \" <vmeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*))litrs*\", \" <vmeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"\\b(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *l+\\b\", \" <vmeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *mls*\", \" <vmeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*)) *ltrs*\", \" <vmeasure> \", string, flags=re.I)\n",
    "    \n",
    "    # Horse power\n",
    "    string = re.sub(r\"\\b(?<![\\dA-Za-z])(\\d)+ *cv\\b\", \" <hpmeasure> \", string, flags=re.I)\n",
    "    string = re.sub(r\"\\b(?<![\\dA-Za-z])((\\d)+(\\,|\\.)*(\\d*))+ *hp\\b\", \" <hpmeasure> \", string, flags=re.I)\n",
    "    \n",
    "    # Time\n",
    "    string = re.sub(r\"\\b(?<![\\dA-Za-z])((\\d)+(\\:)*(\\d*)) *hs*(?![\\d])\\b\", \" <time> \", string, flags=re.I)\n",
    "    \n",
    "    # Quantity\n",
    "    string = re.sub(r\"\\bX\\d{1,4}\\b\", \" <quantity> \", string, flags=re.I)\n",
    "    \n",
    "    # Money\n",
    "    string = re.sub(r\"(?<![\\dA-Za-z])\\$ *((\\d)+(\\,|\\.)*(\\d*))\", \" <money> \", string, flags=re.I)\n",
    "    \n",
    "    # Dimension (could be smeasure too)\n",
    "    string = re.sub(r\"(((\\d)+(\\,|\\.)*(\\d*))+ {0,1}(x|\\*){1} {0,1}((\\d)+(\\,|\\.)*(\\d*))+)\"\n",
    "                    \"+( {0,1}(x|\\*){1} {0,1}((\\d)+(\\,|\\.)*(\\d*))+)*\", \" <dimension> \", string, flags=re.I) \n",
    "    \n",
    "    # Resolution\n",
    "    string = re.sub(r\"\\b(?<![A-Za-z\\-])\\d+p\\b\", \" <res> \", string)\n",
    "    \n",
    "    # Date\n",
    "    string = re.sub(r\"\\b\\d{2}-\\d{2}-(19\\d{2}|20\\d{2})\\b\", \" <date> \", string)\n",
    "\n",
    "    # Model\n",
    "    string = re.sub(r\"(?<!\\d{4})[A-Za-z\\-]+\\d+[A-Za-z\\-\\.0-9]*\", \" <model> \", string, flags=re.I)\n",
    "    string = re.sub(r\"[A-Za-z\\-\\.0-9]*\\d+[A-Za-z\\-](?!\\d{4})\", \" <model> \", string, flags=re.I)\n",
    "    string = re.sub(r\"<model> \\d+\", \" <model> \", string, flags=re.I)\n",
    "    string = re.sub(r\"\\d+ <model>\", \" <model> \", string, flags=re.I)\n",
    "    \n",
    "    # Years\n",
    "    string = re.sub(r\"(?<![A-Za-z0-9])19\\d{2}|20\\d{2}(?![A-Za-z0-9])\", \" <year> \", string)\n",
    "    \n",
    "    # Numbers\n",
    "    string = re.sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" <number> \", string)\n",
    "    \n",
    "    # String cleanup\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"/\", \" / \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\*\", \" * \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"#\\S+\", \" <hashtag> \" , string)\n",
    "    string = re.sub(r\"\\\\\", \" \", string)\n",
    "    string = re.sub(r\"\\+\", \" \", string)\n",
    "    string = re.sub(r\"\\d+\", \" <number> \", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`<>]\", \" \", string, flags=re.I)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f6d56f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Input string: Mesas De Pizarra.\n",
      "Clean string: mesas de pizarra\n",
      "\n",
      " Input string: Termix C6250060 Cepillo Termico Ceramic, 60 Mm \n",
      "Clean string: termix c <longnumber> cepillo termico ceramic , <smeasure>\n",
      "\n",
      " Input string: Titanio Fidget Spinner Blade Pocket Mano Juguete Dedo Giro E\n",
      "Clean string: titanio fidget spinner blade pocket mano juguete dedo giro e\n",
      "\n",
      " Input string: Bomba Embrague Chevrolet Silverado-grand Blazer 3/4\n",
      "Clean string: bomba embrague chevrolet silverado grand blazer <number> <number>\n",
      "\n",
      " Input string: Bolsa De Polietileno En Alta Densidad En 30x40 Por 500 Unid.\n",
      "Clean string: bolsa de polietileno en alta densidad en <dimension> por <number> unid\n"
     ]
    }
   ],
   "source": [
    "for string in train_data.sample(5).title.values:\n",
    "    print(f\"\\n Input string: {string}\")\n",
    "    print(f\"Clean string: {clean_str(string)}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e2cf76",
   "metadata": {},
   "source": [
    "**Apply cleaning to entire dataset**\n",
    "\n",
    "Considering that it is a 20M rows dataset, this process is quite time consuming. That's the reasong why it is performed separately and the results stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56bf9cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01495814323425293,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 62,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 200000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fd24a0009c47028dc06ab764e33434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['clean_title'] = train_data.progress_apply(lambda x : clean_str(x['title']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129282b",
   "metadata": {},
   "source": [
    "**Save cleaned dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "433cd70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('./../data/sample_clean_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e140393",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b8083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_data['clean_title'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4ab001c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012987375259399414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 62,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 200000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984344c20d4b4476a8c6a651b07ef901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now that the text is clean, we can split it into words just by splitting on spaces\n",
    "x_text = [s.split(\" \") for s in tqdm(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6667eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(Phrases(x_text, min_count=100, threshold=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac376a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016952991485595703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 62,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 200000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feae2df88d81493ea7136add12ce2ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_text = [bigram[text] for text in tqdm(x_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4c9ae",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74cc9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentences, lang='english'):\n",
    "    try:\n",
    "        stpwrds = stopwords.words(lang)\n",
    "    except Exception:\n",
    "        stpwrds = stopwords.words('spanish')\n",
    "        \n",
    "    out_sentences = [[w for w in sentence if w not in stpwrds] for sentence in tqdm(sentences)]\n",
    "\n",
    "    return out_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20947890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012963294982910156,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 62,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 200000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2386178d1ab47b8a39406b6e1f705f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_text = remove_stopwords(x_text, lang='spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96255b",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1e4ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stemming(sentences, lang='english'):\n",
    "    try:\n",
    "        stemmer = SnowballStemmer(lang)\n",
    "    except Exception:\n",
    "        stemmer = SnowballStemmer('spanish')\n",
    "    \n",
    "    out_text = [[stemmer.stem(i) for i in text] for text in tqdm(sentences)]\n",
    "\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb721c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01096963882446289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 62,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 200000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142e16097a9e41bf861a30cf75c07118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_text = text_stemming(x_text, lang='spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a03a172",
   "metadata": {},
   "source": [
    "### Pad sentences\n",
    "In order to feed the data into a CNN, all input sentences must have the same length\n",
    "\n",
    "This next step add a padding token or truncate sentences to make all of them of the same lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9085a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sentences, padding_word=\"<PAD/>\", len_sent = None):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    if len_sent is None:\n",
    "        sequence_length = max(len(x) for x in sentences)\n",
    "    else:\n",
    "        sequence_length = len_sent\n",
    "    padded_sentences = []\n",
    "    for i in tqdm(range(len(sentences))):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        if num_padding >= 0:\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "        else:\n",
    "            new_sentence = sentence[:sequence_length]\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b0a107a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01094365119934082,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 62,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 200000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b41b7f73664c73b7234c21b58be9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_text = pad_sentences(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6b90eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uxcell', 'detector', 'interruptor', 'sensor', 'proxim', 'induc', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>']\n"
     ]
    }
   ],
   "source": [
    "print(x_text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5620d9ff",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "For this project I used Word2Vec and now it is a good point to calculate the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aaada95",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "model = Word2Vec(sentences = x_text, vector_size=EMBEDDING_DIM, sg=1, window=7, min_count=20, seed=42, workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3329e886",
   "metadata": {},
   "source": [
    "We should also save the weights and vocabulary to be used later in the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50c2ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.wv.vectors\n",
    "np.save(open('./../data/embeddings.npz', 'wb'), weights)\n",
    "vocab = model.wv.key_to_index \n",
    "with open('./../data/map.json', 'w') as f:\n",
    "    f.write(json.dumps(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b6896",
   "metadata": {},
   "source": [
    "### Build input data\n",
    "\n",
    "Now that we have the veight vectors and the word indices, we can replace each word with an index to be used as an input to the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f58682e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentences_padded'] = x_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05a2c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_path='/map.json', path=''):\n",
    "    \"\"\"\n",
    "    Load word -> index and index -> word mappings\n",
    "    :param vocab_path: where the word-index map is saved\n",
    "    :return: word2idx, idx2word\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path+vocab_path, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    word2idx = data\n",
    "    idx2word = dict([(v, k) for k, v in data.items()])\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0038f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_data(df, path=''):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    word2idx, idx2word = load_vocab(vocab_path='/map.json', path=path)\n",
    "    df['input_data'] = df.progress_apply(lambda x : np.array([word2idx[word] if word in word2idx.keys() else word2idx[\"<PAD/>\"] for word in x['sentences_padded']], dtype=np.int32), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3c6618e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011966705322265625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 62,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 200000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8754d3d7934c6e9766f7eb943cf923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = build_input_data(train_data, path='./../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9767c25",
   "metadata": {},
   "source": [
    "### Split data into train and test\n",
    "Since we are using a 1% of the data only, some categories won't have enough data.\n",
    "\n",
    "We will drop them for now abut this won't be included in the final code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e72c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean title column is no longer needed\n",
    "train_data.drop(columns=['clean_title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7b6086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[~train_data.category.isin(train_data.category.value_counts()[train_data.category.value_counts() < 2].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "109a161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_data,test_size=0.1, stratify=train_data['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cfdc92",
   "metadata": {},
   "source": [
    "### Save the data\n",
    "\n",
    "Save the data to be used in the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "817c5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in train_df['sentences_padded']:\n",
    "    len_sent = len(each)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "993d9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(len_sent, './../data/len_sent.h5')      \n",
    "train_df.to_pickle('./../data/df.pkl')\n",
    "test_df.to_pickle('./../data/df_test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meli2019",
   "language": "python",
   "name": "meli2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
