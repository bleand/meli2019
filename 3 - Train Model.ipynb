{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1363090",
   "metadata": {},
   "source": [
    "## MeLi Data Challenge 2019\n",
    "\n",
    "This notebook is part of a curated version of my original solution for the MeLi Data Challenge hosted by [Mercado Libre](https://www.mercadolibre.com/) in 2019\n",
    "\n",
    "The goal of this first challenge was to create a model that would classify items into categories based solely on the itemâ€™s title. \n",
    "\n",
    "This title is a free text input from the seller that would become the header of the listings.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note</b> <p>Only 10% of the data is used in the notebooks to improve the experience.</p>\n",
    "    <p>Also, data is not being splitted by language in this notebooks for simplicity reasons only</p>\n",
    "    <p>In the scripted version, 100% of the data is used to improve results</p>\n",
    "</div>\n",
    "\n",
    "### 2 - Train Model\n",
    "\n",
    "In this notebook, we train a CNN using all the data created in the previous steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b3d89",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fcb4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63185c",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aad3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/df.pkl')\n",
    "len_sent = joblib.load('./data/len_sent.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64075504",
   "metadata": {},
   "source": [
    "### Encode cateogories and save the references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae9a69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/levels']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classes = len(np.unique(df['category']))\n",
    "labels, levels = pd.factorize(df['category'])        \n",
    "joblib.dump(nb_classes,'./data/nb_classes')\n",
    "joblib.dump(levels,'./data/levels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93efd176",
   "metadata": {},
   "source": [
    "### Split data \n",
    "\n",
    "We now split data into train and validation\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note</b> <p>In <b>2- PreProcess</b> we also splitted the data in order to have a testing set</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d50a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df,test_size=0.1, stratify=df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d04d14",
   "metadata": {},
   "source": [
    "### Data preparation steps\n",
    "Here, we extract the values from the dataframes and generate the necessary encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55172c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = copy.deepcopy(train_df['category'].values)\n",
    "x = copy.deepcopy(train_df['input_data'].values)\n",
    "y_val_in = copy.deepcopy(val_df['category'].values)\n",
    "x_val_in = copy.deepcopy(val_df['input_data'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff1376cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_one_hot(data, nb_classes):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    targets = np.array(data, dtype=np.int16).reshape(-1)\n",
    "    return np.eye(nb_classes,dtype=np.int8)[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5e9f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [np.where(levels==i)[0][0] for i in y]\n",
    "y_val = [np.where(levels==i)[0][0] for i in y_val_in]\n",
    "\n",
    "y = indices_to_one_hot(y, nb_classes)\n",
    "y_val = indices_to_one_hot(y_val, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d4ea3",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5b495a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape = y_val.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d988492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(input_dim, output_shape, path=''):\n",
    "    \n",
    "    weights = np.load(open(path+'/embeddings.npz', 'rb'))\n",
    "    embedding_dim = weights.shape[1]\n",
    "    \n",
    "    inputs = Input(shape=(input_dim,), dtype='int32')\n",
    "    \n",
    "    embedding = Embedding(output_dim=weights.shape[1], input_dim=weights.shape[0], input_length=input_dim,\n",
    "                              weights=[weights], trainable=True)(inputs)\n",
    "                              \n",
    "    spatial_dropout = SpatialDropout1D(0.5)(embedding)\n",
    "        \n",
    "    reshape = Reshape((input_dim, embedding_dim, 1))(spatial_dropout)\n",
    "\n",
    "    conv_0 = Conv2D(num_filters, (filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal',\n",
    "                           activation='sigmoid', data_format='channels_last')(reshape)\n",
    "    conv_1 = Conv2D(num_filters, (filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal',\n",
    "                           activation='sigmoid', data_format='channels_last')(reshape)\n",
    "    conv_2 = Conv2D(num_filters, (filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal',\n",
    "                           activation='sigmoid', data_format='channels_last')(reshape)\n",
    "    conv_3 = Conv2D(num_filters, (filter_sizes[3], embedding_dim), padding='valid', kernel_initializer='normal',\n",
    "                       activation='sigmoid', data_format='channels_last')(reshape)\n",
    "\n",
    "\n",
    "    maxpool_0 = MaxPooling2D(pool_size=(input_dim - filter_sizes[0] + 1, 1), strides=(1, 1),\n",
    "                             padding='valid', data_format='channels_last')(conv_0)\n",
    "    maxpool_1 = MaxPooling2D(pool_size=(input_dim - filter_sizes[1] + 1, 1), strides=(1, 1),\n",
    "                             padding='valid', data_format='channels_last')(conv_1)\n",
    "    maxpool_2 = MaxPooling2D(pool_size=(input_dim - filter_sizes[2] + 1, 1), strides=(1, 1),\n",
    "                             padding='valid', data_format='channels_last')(conv_2)\n",
    "    maxpool_3 = MaxPooling2D(pool_size=(input_dim - filter_sizes[3] + 1, 1), strides=(1, 1),\n",
    "                             padding='valid', data_format='channels_last')(conv_3)\n",
    "\n",
    "    merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2, maxpool_3], axis=1)\n",
    "    \n",
    "    flatten = Flatten()(merged_tensor)\n",
    "    \n",
    "    #dense1 = Dense(units=output_dim, kernel_regularizer=regularizers.l2(0.01))(flatten)\n",
    "    dense1 = Dense(units=output_dim)(flatten)\n",
    "    #dense1 = BatchNormalization()(dense1)\n",
    "    dense1 = Activation('relu')(dense1)\n",
    "    dense1 = Dropout(drop)(dense1)\n",
    "\n",
    "    #dense2 = Dense(units=output_dim, kernel_regularizer=regularizers.l2(0.01))(dense1)\n",
    "    dense2 = Dense(units=output_dim)(dense1)\n",
    "    #dense2 = BatchNormalization()(dense2)\n",
    "    dense2 = Activation('relu')(dense2)\n",
    "    dense2 = Dropout(drop)(dense2)\n",
    "\n",
    "    #dense2 = Dense(units=output_dim, activation='relu')(dense1)\n",
    "    #dense3 = Dense(units=output_dim, activation='relu')(dense2)\n",
    "    output = Dense(units=output_shape)(dense1)\n",
    "    #output = BatchNormalization()(output)\n",
    "    output = Activation('softmax')(output)\n",
    "\n",
    "\n",
    "    #output = Dense(units=output_shape, activation='softmax')(normalized_1)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_model(len_sent, output_shape, path='data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meli2019",
   "language": "python",
   "name": "meli2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
